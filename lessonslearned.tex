\section{Lessons from Applying the Selected Practice}
\label{sec:Lessons from Applying the Selected Practice}

Despite my initial plan being to set up and host most of my Continuous
Integration infrastructure myself, it became clear after an initial trial
that it would be far easier for me to ensure build repeatability/speed if I used
a cloud-based CI service (some of the reasons for which are detailed below).
With this in mind, I elected to use the popular and
excellent \href{https://travis-ci.org/}{Travis CI} - which provides a free tier
for open-source projects such as mine. Setup was as simple as adding a
`.travis.yml` file to my project, and enabling my GitHub repo for builds on
the Travis.ci admin panel. My initial .travis.yml file can be seen in
Listing~\ref{lst:initialTravis}, and the latest version is available
\href{https://github.com/FireEater64/gamq/blob/master/.travis.yml}{on GitHub}.
There are a multitude of configurable options available, but
my initial, basic configuration in Listing~\ref{lst:initialTravis} consists of
only three:

\begin{description}
  \item[Language] Defines the language of the project being built - in this case
  \href{https://golang.org/}{GoLang}. TravisCI builds (usually) take place
  inside \href{https://www.docker.com/what-docker}{Docker containers}, with the
  'language' section of config dictating which pre-built container is used for
  this particular build. In this case, a language of 'go' will ensure that the
  build container contains all of the binaries and environment variables required
  for building and running go projects.
  \item[Go] This section defines different versions of the go compiler you wish
  to build your product on. When code is checked in - Travis will spin up a
  separate container for each version specified, and run your complete test suite
  inside each container in parallel. This feature is known as the
  '\href{https://docs.travis-ci.com/user/customizing-the-build/#Build-Matrix}{build matrix}',
  and is both \emph{incredibly} useful for ensuring software consistency on multiple
  different compilers/runtimes (especially useful for interpreted languages such
  as Java), and something which would be hard to replicate outside of a
  containerised build environment (i.e. if I'd chosen to self-host my CI).
  \item[Script] Any custom commands you wish to be run as part of the build.
  Travis contains (pun intended) a standard build script for most languages (which
  are selected via the 'language' configuration detailed above), however builds
  invariably require additional scripts/commands to be run as well - some example
  use cases could be to run additional tests, or deploy build binaries to an
  artifact repository. In this case, the commands I specified execute both my
  unit/integration test suite, and my benchmark suite (Detailed below).
\end{description}

More information on the contents of .travis.yml files can be found in their
\href{https://docs.travis-ci.com/}{excellent documentation}.

\begin{listing}[ht]
\begin{minted}{YAML}
language: go

go:
  - 1.3
  - 1.4
  - tip

script: go test -v ./... -bench=.
\end{minted}
\caption{Initial .travis.yml}
\label{lst:initialTravis}
\end{listing}

Of course, this is only half the story. Using Continuous Integration to ensure
that software builds successfully, whilst useful, does not ensure the correctness
of the software being built. That requires a battery of tests, which can be run
on each successful build, to help ensure the correctness of my project - as well
as help guard against possible regression. The unit/integration/benchmark tests
for my project can be viewed in
\href{https://github.com/FireEater64/gamq}{my projects Git repository}, and uses
the GoLang convention of treating all source files of pattern '*\_test.go' as test
files. The specifics of my test harness are outside the scope of my report, which
deals with my use of Continuous Integration - as a result, only a brief summery of
tests is given.

My tests fall into two main categories, and use the standard, built in testing
features of Go:

\subsection{Unit/Integration Tests}
\label{sub:Unit/Integration Tests}

Unit/integration tests in go are written in much the same as any other language
(see Listing~\ref{lst:goUnitTest}) - and afford me protection against code that
fails to meet specification, as well as regressions in functionality resulting
from code changes/re-factoring. One important (though not definitive) metric
associated with unit/integration tests is \emph{code coverage}, which is the
total percentage of the code-base which is executed by running unit tests. This
is a useful metric to keep an eye on, as it helps\footnote{Though doesn't always}
indicate which sections of code are susceptible to bugs as a result of not being
tested. Go's built in test runner is capable of producing code coverage reports
during test runs, however I chose to use an open source tool called '\href{}{gocov}',
as it allowed me to send the code quality metrics produced my build to another
online service, \href{https://coveralls.io/}{coveralls}. The reasons behind doing
this, rather than relying on the build-in HTML report were:

\begin{description}
  \item[Visibility] Publishing my code quality metrics in an easily accessibly,
  publicly visible website (as well as the front page of my GitHub project),
  rather than hiding them away in build logs helps to
  'keep me honest', as well as 'gamify' the process of driving up coverage.
  \item[Monitoring] Coveralls allows me to set 'thresholds' for coverage metrics,
  and will fail the build if these are not adhered to. For example, my build will
  fail if the coverage of the checked in code is even 0.1\% less than that of the
  previous successful build.
\end{description}

The code quality metrics for my project are available
\href{https://coveralls.io/github/FireEater64/gamq?branch=master}{on Coveralls},
as well as being summarised in the
\href{https://github.com/FireEater64/gamq/blob/master/README.md}{README for my project on GitHub}.

\begin{listing}[ht]
\begin{minted}{Go}
  func Test(t *testing.T) {
  	underTest := MyClass{}
  	result := underTest.SomeFunction()

  	if result != expected {
  		t.Fail()
  	}
  }
\end{minted}
\caption{An example of a unit test in Go}
\label{lst:goUnitTest}
\end{listing}
